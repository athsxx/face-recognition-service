{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Service - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the Face Recognition Service capabilities including:\n",
    "- Face detection\n",
    "- Face alignment\n",
    "- Embedding extraction\n",
    "- Identity matching\n",
    "- Gallery management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add FRS to path\n",
    "sys.path.append('.')\n",
    "\n",
    "from frs.core.detector import FaceDetector\n",
    "from frs.core.alignment import FaceAligner\n",
    "from frs.core.embedding import FaceEmbedding\n",
    "from frs.core.matcher import FaceMatcher\n",
    "from frs.database.models import Database\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "print(\"Loading face detector...\")\n",
    "detector = FaceDetector()\n",
    "print(\"✓ Detector loaded\")\n",
    "\n",
    "# Initialize aligner\n",
    "print(\"Loading face aligner...\")\n",
    "aligner = FaceAligner()\n",
    "print(\"✓ Aligner loaded\")\n",
    "\n",
    "# Initialize embedder\n",
    "print(\"Loading embedding model...\")\n",
    "embedder = FaceEmbedding()\n",
    "print(\"✓ Embedder loaded\")\n",
    "\n",
    "# Initialize database and matcher\n",
    "print(\"Setting up database and matcher...\")\n",
    "db = Database(\"sqlite:///data/demo.db\")\n",
    "db.create_tables()\n",
    "matcher = FaceMatcher(db)\n",
    "print(\"✓ Database and matcher ready\")\n",
    "\n",
    "print(\"\\n✓ All components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Face Detection Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(image_path):\n",
    "    \"\"\"Detect and visualize faces in an image.\"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Detect faces\n",
    "    start = time.time()\n",
    "    faces = detector.detect(image)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"Detected {len(faces)} face(s) in {elapsed:.2f}ms\")\n",
    "    \n",
    "    # Draw results\n",
    "    result_img = detector.draw_detections(image, faces)\n",
    "    \n",
    "    # Convert BGR to RGB for display\n",
    "    result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(result_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Face Detection Result: {len(faces)} face(s) detected')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print details\n",
    "    for i, face in enumerate(faces):\n",
    "        print(f\"\\nFace {i+1}:\")\n",
    "        print(f\"  BBox: {face['bbox']}\")\n",
    "        print(f\"  Confidence: {face['confidence']:.3f}\")\n",
    "        print(f\"  Quality Score: {face.get('quality_score', 0):.3f}\")\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# Example usage (replace with your image path)\n",
    "# faces = visualize_detection('path/to/your/image.jpg')\n",
    "print(\"Function defined. Use: visualize_detection('image_path.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Face Alignment Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_alignment(image_path):\n",
    "    \"\"\"Show before/after face alignment.\"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    faces = detector.detect(image)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected\")\n",
    "        return\n",
    "    \n",
    "    # Align first face\n",
    "    face = faces[0]\n",
    "    aligned = aligner.align(image, face['landmarks'])\n",
    "    \n",
    "    # Extract face region\n",
    "    x1, y1, x2, y2 = face['bbox']\n",
    "    face_crop = image[y1:y2, x1:x2]\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Face Crop')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Aligned\n",
    "    axes[1].imshow(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title('Aligned Face (112x112)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "# Example usage\n",
    "# aligned_face = show_alignment('image.jpg')\n",
    "print(\"Function defined. Use: show_alignment('image_path.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_analyze_embedding(image_path):\n",
    "    \"\"\"Extract embedding and analyze its properties.\"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    faces = detector.detect(image)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces detected\")\n",
    "        return None\n",
    "    \n",
    "    # Align and extract embedding\n",
    "    aligned = aligner.align(image, faces[0]['landmarks'])\n",
    "    \n",
    "    start = time.time()\n",
    "    embedding = embedder.extract(aligned)\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"Embedding extraction: {elapsed:.2f}ms\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"Embedding norm (should be ~1.0): {np.linalg.norm(embedding):.4f}\")\n",
    "    print(f\"Mean: {embedding.mean():.4f}, Std: {embedding.std():.4f}\")\n",
    "    \n",
    "    # Visualize embedding distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(embedding, bins=50)\n",
    "    plt.title('Embedding Value Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(embedding)\n",
    "    plt.title('Embedding Values (512 dimensions)')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Example usage\n",
    "# emb = extract_and_analyze_embedding('image.jpg')\n",
    "print(\"Function defined. Use: extract_and_analyze_embedding('image_path.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gallery Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_to_gallery(image_path, name, identity_id=None):\n",
    "    \"\"\"Add a person to the recognition gallery.\"\"\"\n",
    "    import uuid\n",
    "    \n",
    "    if identity_id is None:\n",
    "        identity_id = f\"id_{uuid.uuid4().hex[:12]}\"\n",
    "    \n",
    "    image = cv2.imread(str(image_path))\n",
    "    faces = detector.detect(image)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"❌ No face detected\")\n",
    "        return False\n",
    "    \n",
    "    if len(faces) > 1:\n",
    "        print(f\"⚠️  Multiple faces detected ({len(faces)}), using first one\")\n",
    "    \n",
    "    # Extract embedding\n",
    "    aligned = aligner.align(image, faces[0]['landmarks'])\n",
    "    embedding = embedder.extract(aligned)\n",
    "    \n",
    "    # Add to gallery\n",
    "    success = matcher.add_identity(\n",
    "        identity_id=identity_id,\n",
    "        name=name,\n",
    "        embedding=embedding,\n",
    "        image_path=str(image_path)\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"✓ Added {name} to gallery (ID: {identity_id})\")\n",
    "        print(f\"  Gallery size: {len(matcher.get_all_identities())}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to add {name}\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Example usage\n",
    "# add_person_to_gallery('person1.jpg', 'John Doe')\n",
    "# add_person_to_gallery('person2.jpg', 'Jane Smith')\n",
    "print(\"Function defined. Use: add_person_to_gallery('image.jpg', 'Person Name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces(image_path, top_k=5):\n",
    "    \"\"\"Recognize faces in an image.\"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = detector.detect(image)\n",
    "    print(f\"Detected {len(faces)} face(s)\")\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return\n",
    "    \n",
    "    # Recognize each face\n",
    "    result_img = image.copy()\n",
    "    \n",
    "    for i, face in enumerate(faces):\n",
    "        # Extract embedding\n",
    "        aligned = aligner.align(image, face['landmarks'])\n",
    "        embedding = embedder.extract(aligned)\n",
    "        \n",
    "        # Match\n",
    "        matches = matcher.match(embedding)\n",
    "        \n",
    "        # Draw results\n",
    "        x1, y1, x2, y2 = face['bbox']\n",
    "        \n",
    "        if matches:\n",
    "            best_match = matches[0]\n",
    "            name = best_match['name']\n",
    "            confidence = best_match['confidence']\n",
    "            color = (0, 255, 0) if confidence > 0.6 else (0, 165, 255)\n",
    "            label = f\"{name} ({confidence:.2f})\"\n",
    "        else:\n",
    "            color = (0, 0, 255)\n",
    "            label = \"Unknown\"\n",
    "        \n",
    "        # Draw box and label\n",
    "        cv2.rectangle(result_img, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(result_img, label, (x1, y1-10), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        # Print matches\n",
    "        print(f\"\\nFace {i+1}:\")\n",
    "        if matches:\n",
    "            print(f\"  Top matches:\")\n",
    "            for j, match in enumerate(matches[:top_k]):\n",
    "                print(f\"    {j+1}. {match['name']}: {match['confidence']:.3f}\")\n",
    "        else:\n",
    "            print(\"  No matches found\")\n",
    "    \n",
    "    # Display\n",
    "    result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(result_img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Face Recognition Results')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# recognize_faces('test_image.jpg')\n",
    "print(\"Function defined. Use: recognize_faces('image_path.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pipeline(image_path, num_iterations=10):\n",
    "    \"\"\"Benchmark the end-to-end pipeline.\"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    \n",
    "    times = {\n",
    "        'detection': [],\n",
    "        'alignment': [],\n",
    "        'embedding': [],\n",
    "        'matching': [],\n",
    "        'total': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Running {num_iterations} iterations...\")\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        start_total = time.time()\n",
    "        \n",
    "        # Detection\n",
    "        start = time.time()\n",
    "        faces = detector.detect(image)\n",
    "        times['detection'].append((time.time() - start) * 1000)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Alignment\n",
    "        start = time.time()\n",
    "        aligned = aligner.align(image, faces[0]['landmarks'])\n",
    "        times['alignment'].append((time.time() - start) * 1000)\n",
    "        \n",
    "        # Embedding\n",
    "        start = time.time()\n",
    "        embedding = embedder.extract(aligned)\n",
    "        times['embedding'].append((time.time() - start) * 1000)\n",
    "        \n",
    "        # Matching\n",
    "        start = time.time()\n",
    "        _ = matcher.match(embedding)\n",
    "        times['matching'].append((time.time() - start) * 1000)\n",
    "        \n",
    "        times['total'].append((time.time() - start_total) * 1000)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE BENCHMARK RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for component, timings in times.items():\n",
    "        if timings:\n",
    "            avg = np.mean(timings)\n",
    "            std = np.std(timings)\n",
    "            min_t = np.min(timings)\n",
    "            max_t = np.max(timings)\n",
    "            fps = 1000 / avg if avg > 0 else 0\n",
    "            \n",
    "            print(f\"\\n{component.upper()}:\")\n",
    "            print(f\"  Average: {avg:.2f}ms ± {std:.2f}ms\")\n",
    "            print(f\"  Min/Max: {min_t:.2f}ms / {max_t:.2f}ms\")\n",
    "            print(f\"  Throughput: {fps:.2f} FPS\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    components = ['Detection', 'Alignment', 'Embedding', 'Matching']\n",
    "    avg_times = [np.mean(times[k]) for k in ['detection', 'alignment', 'embedding', 'matching']]\n",
    "    \n",
    "    ax1.bar(components, avg_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax1.set_ylabel('Latency (ms)')\n",
    "    ax1.set_title('Average Latency by Component')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(avg_times, labels=components, autopct='%1.1f%%', startangle=90,\n",
    "           colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax2.set_title('Latency Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Example usage\n",
    "# benchmark_results = benchmark_pipeline('image.jpg', num_iterations=20)\n",
    "print(\"Function defined. Use: benchmark_pipeline('image.jpg', num_iterations=20)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete End-to-End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow:\n",
    "# 1. Add people to gallery\n",
    "# add_person_to_gallery('data/person1.jpg', 'Person 1')\n",
    "# add_person_to_gallery('data/person2.jpg', 'Person 2')\n",
    "\n",
    "# 2. List gallery\n",
    "identities = matcher.get_all_identities()\n",
    "print(f\"Gallery contains {len(identities)} identities:\")\n",
    "for identity in identities:\n",
    "    print(f\"  - {identity['name']} (ID: {identity['identity_id']})\")\n",
    "\n",
    "# 3. Recognize faces in new image\n",
    "# recognize_faces('data/test_image.jpg')\n",
    "\n",
    "# 4. Benchmark performance\n",
    "# results = benchmark_pipeline('data/test_image.jpg', num_iterations=10)\n",
    "\n",
    "print(\"\\n✓ Ready to run examples!\")\n",
    "print(\"\\nQuick start:\")\n",
    "print(\"1. add_person_to_gallery('your_image.jpg', 'Name')\")\n",
    "print(\"2. recognize_faces('test_image.jpg')\")\n",
    "print(\"3. benchmark_pipeline('test_image.jpg')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
